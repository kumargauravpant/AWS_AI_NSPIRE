{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import truecase\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Hello. My name is Kirsten Freeman. I'm originally from Denver colorado but was raised right here in Knoxville Tennessee. Today I'll be sharing a little bit more about my background, my interest in the holidays and my plans for the future. As I mentioned before, I was raised in Knoxville. My elementary middle and high school are all within a 15 mile radius of the University of Tennessee. I've been playing sports since I was six years old, which was a crucial part of my childhood and helped me become who I am today. I'm also a very competitive person. This helped bring a sense of selflessness at a very young age because I was willing to sacrifice anything to win games and reach my desired and result this sense of selflessness would help me in the future. When I became more interested in service clubs, my interest in entering the service groups only increases I would throughout school, by my senior in high school. This interest had turned into passion. I was involved with five service clubs within my school as well as to other in the community. I received most school service principles award, a service to humanities award and other recognitions given to me by my school as well as my community for my efforts and passion in school and community service. Lastly, I would like to share my plans for the future. I'm a history major with a minor and secondary education. I plan on becoming a history teacher and participating in the teachers for America program, which combines my love of history teaching and helping others. My end goal, however, is to become a high school principal in this position, I would love to remain heavily involved with student life. So today I've shared with you a little bit more about myself, my interest and hobbies and finally, my plans for the future. Thank you.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello. My name is Kirsten Freeman. I'm originally from Denver colorado but was raised right here in Knoxville Tennessee. Today I'll be sharing a little bit more about my background, my interest in the holidays and my plans for the future. As I mentioned before, I was raised in Knoxville. My elementary middle and high school are all within a 15 mile radius of the University of Tennessee. I've been playing sports since I was six years old, which was a crucial part of my childhood and helped me become who I am today. I'm also a very competitive person. This helped bring a sense of selflessness at a very young age because I was willing to sacrifice anything to win games and reach my desired and result this sense of selflessness would help me in the future. When I became more interested in service clubs, my interest in entering the service groups only increases I would throughout school, by my senior in high school. This interest had turned into passion. I was involved with five service clubs within my school as well as to other in the community. I received most school service principles award, a service to humanities award and other recognitions given to me by my school as well as my community for my efforts and passion in school and community service. Lastly, I would like to share my plans for the future. I'm a history major with a minor and secondary education. I plan on becoming a history teacher and participating in the teachers for America program, which combines my love of history teaching and helping others. My end goal, however, is to become a high school principal in this position, I would love to remain heavily involved with student life. So today I've shared with you a little bit more about myself, my interest and hobbies and finally, my plans for the future. Thank you.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4bdf538c544f92b19ca43ac9e7bd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1199.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae89bb7d33944e8eb686de1a0ffdbce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691430.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')  #t5-base  is a pretrained model.\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-large')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2b8a36f04d4a148cb347338e09a4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9544122ba2942a789caea08d2ed5312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " Hello. My name is Kirsten Freeman. I'm originally from Denver colorado but was raised right here in Knoxville Tennessee. Today I'll be sharing a little bit more about my background, my interest in the holidays and my plans for the future. As I mentioned before, I was raised in Knoxville. My elementary middle and high school are all within a 15 mile radius of the University of Tennessee. I've been playing sports since I was six years old, which was a crucial part of my childhood and helped me become who I am today. I'm also a very competitive person. This helped bring a sense of selflessness at a very young age because I was willing to sacrifice anything to win games and reach my desired and result this sense of selflessness would help me in the future. When I became more interested in service clubs, my interest in entering the service groups only increases I would throughout school, by my senior in high school. This interest had turned into passion. I was involved with five service clubs within my school as well as to other in the community. I received most school service principles award, a service to humanities award and other recognitions given to me by my school as well as my community for my efforts and passion in school and community service. Lastly, I would like to share my plans for the future. I'm a history major with a minor and secondary education. I plan on becoming a history teacher and participating in the teachers for America program, which combines my love of history teaching and helping others. My end goal, however, is to become a high school principal in this position, I would love to remain heavily involved with student life. So today I've shared with you a little bit more about myself, my interest and hobbies and finally, my plans for the future. Thank you.\n"
     ]
    }
   ],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\",\"\")  #remove newlines and append space\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text \n",
    "print (\"original text preprocessed: \\n\", preprocess_text)\n",
    "\n",
    "# tokenized_text = tokenizer.encode(t5_prepared_Text,return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(t5_prepared_Text,return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21603,    10,  8774,     5,   499,   564,    19, 10976,  1913,  1443,\n",
       "           348,     5,    27,    31,    51,  5330,    45, 12154,   945,     9,\n",
       "            26,    32,    68,    47,  3279,   269,   270,    16, 25649,  1420,\n",
       "         12976,     5,  1960,    27,    31,   195,    36,  2178,     3,     9,\n",
       "           385,   720,    72,    81,    82,  2458,     6,    82,  1046,    16,\n",
       "             8,  6799,    11,    82,  1390,    21,     8,   647,     5,   282,\n",
       "            27,  2799,   274,     6,    27,    47,  3279,    16, 25649,  1420,\n",
       "             5,   499, 15468,  2214,    11,   306,   496,    33,    66,   441,\n",
       "             3,     9,   627,  7728, 24153,    13,     8,   636,    13, 12976,\n",
       "             5,    27,    31,   162,   118,  1556,  2100,   437,    27,    47,\n",
       "          1296,   203,   625,     6,    84,    47,     3,     9,  4462,   294,\n",
       "            13,    82,  7352,    11,  2139,   140,   582,   113,    27,   183,\n",
       "           469,     5,    27,    31,    51,    92,     3,     9,   182,  3265,\n",
       "           568,     5,   100,  2139,   830,     3,     9,  1254,    13,  1044,\n",
       "         15543,    44,     3,     9,   182,  1021,  1246,   250,    27,    47,\n",
       "          4403,    12, 10811,   959,    12,  1369,  1031,    11,  1535,    82,\n",
       "          5327,    11,   741,    48,  1254,    13,  1044, 15543,   133,   199,\n",
       "           140,    16,     8,   647,     5,   366,    27,  1632,    72,  1638,\n",
       "            16,   313,  8122,     6,    82,  1046,    16,  7084,     8,   313,\n",
       "          1637,   163,  5386,    27,   133,  1019,   496,     6,    57,    82,\n",
       "          2991,    16,   306,   496,     5,   100,  1046,   141,  2120,   139,\n",
       "          2987,     5,    27,    47,  1381,    28,   874,   313,  8122,   441,\n",
       "            82,   496,    38,   168,    38,    12,   119,    16,     8,   573,\n",
       "             5,    27,  1204,   167,   496,   313,  5559,  2760,     6,     3,\n",
       "             9,   313,    12,   936,  2197,  2760,    11,   119,  5786,     7,\n",
       "           787,    12,   140,    57,    82,   496,    38,   168,    38,    82,\n",
       "           573,    21,    82,  2231,    11,  2987,    16,   496,    11,   573,\n",
       "           313,     5,     3, 19807,     6,    27,   133,   114,    12,   698,\n",
       "            82,  1390,    21,     8,   647,     5,    27,    31,    51,     3,\n",
       "             9,   892,   779,    28,     3,     9,  4012,    11,  6980,  1073,\n",
       "             5,    27,   515,    30,  2852,     3,     9,   892,  3145,    11,\n",
       "          7448,    16,     8,  3081,    21,  1371,   478,     6,    84,     3,\n",
       "         15256,    82,   333,    13,   892,  2119,    11,  2022,   717,     5,\n",
       "           499,   414,  1288,     6,   983,     6,    19,    12,   582,     3,\n",
       "             9,   306,   496,  3218,    16,    48,  1102,     6,    27,   133,\n",
       "           333,    12,  2367,  8672,  1381,    28,  1236,   280,     5,   264,\n",
       "           469,    27,    31,   162,  2471,    28,    25,     3,     9,   385,\n",
       "           720,    72,    81,  1512,     6,    82,  1046,    11, 27781,    11,\n",
       "          2031,     6,    82,  1390,    21,     8,   647,     5,  1562,    25,\n",
       "             5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation_utils:\n",
      "\n",
      "generate(input_ids: Union[torch.LongTensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, encoder_no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, max_time: Union[float, NoneType] = None, max_new_tokens: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, forced_bos_token_id: Union[int, NoneType] = None, forced_eos_token_id: Union[int, NoneType] = None, remove_invalid_values: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor] method of transformers.models.t5.modeling_t5.T5ForConditionalGeneration instance\n",
      "    Generates sequences for models with a language modeling head. The method currently supports greedy decoding,\n",
      "    multinomial sampling, beam-search decoding, and beam-search multinomial sampling.\n",
      "    \n",
      "    Apart from :obj:`input_ids` and :obj:`attention_mask`, all the arguments below will default to the value of the\n",
      "    attribute of the same name inside the :class:`~transformers.PretrainedConfig` of the model. The default values\n",
      "    indicated are the default values of those config.\n",
      "    \n",
      "    Most of these parameters are explained in more detail in `this blog post\n",
      "    <https://huggingface.co/blog/how-to-generate>`__.\n",
      "    \n",
      "    Parameters:\n",
      "    \n",
      "        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            The sequence used as a prompt for the generation. If :obj:`None` the method initializes it with\n",
      "            :obj:`bos_token_id` and a batch size of 1.\n",
      "        max_length (:obj:`int`, `optional`, defaults to :obj:`model.config.max_length`):\n",
      "            The maximum length of the sequence to be generated.\n",
      "        max_new_tokens (:obj:`int`, `optional`, defaults to None):\n",
      "            The maximum numbers of tokens to generate, ignore the current number of tokens. Use either\n",
      "            :obj:`max_new_tokens` or :obj:`max_length` but not both, they serve the same purpose.\n",
      "        min_length (:obj:`int`, `optional`, defaults to 10):\n",
      "            The minimum length of the sequence to be generated.\n",
      "        do_sample (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "        early_stopping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether to stop the beam search when at least ``num_beams`` sentences are finished per batch or not.\n",
      "        num_beams (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of beams for beam search. 1 means no beam search.\n",
      "        temperature (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            The value used to module the next token probabilities.\n",
      "        top_k (:obj:`int`, `optional`, defaults to 50):\n",
      "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "        top_p (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            If set to float < 1, only the most probable tokens with probabilities that add up to :obj:`top_p` or\n",
      "            higher are kept for generation.\n",
      "        repetition_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            The parameter for repetition penalty. 1.0 means no penalty. See `this paper\n",
      "            <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details.\n",
      "        pad_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `padding` token.\n",
      "        bos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `beginning-of-sequence` token.\n",
      "        eos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the `end-of-sequence` token.\n",
      "        length_penalty (:obj:`float`, `optional`, defaults to 1.0):\n",
      "            Exponential penalty to the length. 1.0 means no penalty. Set to values < 1.0 in order to encourage the\n",
      "            model to generate shorter sequences, to a value > 1.0 in order to encourage the model to produce longer\n",
      "            sequences.\n",
      "        no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to int > 0, all ngrams of that size can only occur once.\n",
      "        encoder_no_repeat_ngram_size (:obj:`int`, `optional`, defaults to 0):\n",
      "            If set to int > 0, all ngrams of that size that occur in the ``encoder_input_ids`` cannot occur in the\n",
      "            ``decoder_input_ids``.\n",
      "        bad_words_ids(:obj:`List[List[int]]`, `optional`):\n",
      "            List of token ids that are not allowed to be generated. In order to get the tokens of the words that\n",
      "            should not appear in the generated text, use :obj:`tokenizer(bad_word,\n",
      "            add_prefix_space=True).input_ids`.\n",
      "        num_return_sequences(:obj:`int`, `optional`, defaults to 1):\n",
      "            The number of independently computed returned sequences for each element in the batch.\n",
      "        max_time(:obj:`float`, `optional`, defaults to None):\n",
      "            The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
      "            finish the current pass after allocated time has been passed.\n",
      "        attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values are in ``[0, 1]``, 1 for\n",
      "            tokens that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same\n",
      "            shape as :obj:`input_ids` that masks the pad token. `What are attention masks?\n",
      "            <../glossary.html#attention-mask>`__\n",
      "        decoder_start_token_id (:obj:`int`, `optional`):\n",
      "            If an encoder-decoder model starts decoding with a different token than `bos`, the id of that token.\n",
      "        use_cache: (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "            speed up decoding.\n",
      "        num_beam_groups (:obj:`int`, `optional`, defaults to 1):\n",
      "            Number of groups to divide :obj:`num_beams` into in order to ensure diversity among different groups of\n",
      "            beams. `this paper <https://arxiv.org/pdf/1610.02424.pdf>`__ for more details.\n",
      "        diversity_penalty (:obj:`float`, `optional`, defaults to 0.0):\n",
      "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
      "            at a particular time. Note that :obj:`diversity_penalty` is only effective if ``group beam search`` is\n",
      "            enabled.\n",
      "        prefix_allowed_tokens_fn: (:obj:`Callable[[int, torch.Tensor], List[int]]`, `optional`):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID :obj:`batch_id` and\n",
      "            :obj:`input_ids`. It has to return a list with the allowed tokens for the next generation step\n",
      "            conditioned on the batch ID :obj:`batch_id` and the previously generated tokens :obj:`inputs_ids`. This\n",
      "            argument is useful for constrained generation conditioned on the prefix, as described in\n",
      "            `Autoregressive Entity Retrieval <https://arxiv.org/abs/2010.00904>`__.\n",
      "        output_attentions (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
      "            returned tensors for more details.\n",
      "        output_hidden_states (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
      "            for more details.\n",
      "        output_scores (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return the prediction scores. See ``scores`` under returned tensors for more details.\n",
      "        return_dict_in_generate (:obj:`bool`, `optional`, defaults to `False`):\n",
      "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
      "        forced_bos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the token to force as the first generated token after the :obj:`decoder_start_token_id`.\n",
      "            Useful for multilingual models like :doc:`mBART <../model_doc/mbart>` where the first generated token\n",
      "            needs to be the target language token.\n",
      "        forced_eos_token_id (:obj:`int`, `optional`):\n",
      "            The id of the token to force as the last generated token when :obj:`max_length` is reached.\n",
      "        remove_invalid_values (:obj:`bool`, `optional`):\n",
      "            Whether to remove possible `nan` and `inf` outputs of the model to prevent the generation method to\n",
      "            crash. Note that using ``remove_invalid_values`` can slow down generation.\n",
      "        synced_gpus (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
      "            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
      "    \n",
      "        model_kwargs:\n",
      "            Additional model specific kwargs will be forwarded to the :obj:`forward` function of the model. If the\n",
      "            model is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific\n",
      "            kwargs should be prefixed with `decoder_`.\n",
      "    \n",
      "    Return:\n",
      "        :class:`~transformers.file_utils.ModelOutput` or :obj:`torch.LongTensor`: A\n",
      "        :class:`~transformers.file_utils.ModelOutput` (if ``return_dict_in_generate=True`` or when\n",
      "        ``config.return_dict_in_generate=True``) or a :obj:`torch.FloatTensor`.\n",
      "    \n",
      "            If the model is `not` an encoder-decoder model (``model.config.is_encoder_decoder=False``), the\n",
      "            possible :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchDecoderOnlyOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleDecoderOnlyOutput`\n",
      "    \n",
      "            If the model is an encoder-decoder model (``model.config.is_encoder_decoder=True``), the possible\n",
      "            :class:`~transformers.file_utils.ModelOutput` types are:\n",
      "    \n",
      "                - :class:`~transformers.generation_utils.GreedySearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.SampleEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSearchEncoderDecoderOutput`,\n",
      "                - :class:`~transformers.generation_utils.BeamSampleEncoderDecoderOutput`\n",
      "    \n",
      "    Examples::\n",
      "        >>> from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> # do greedy decoding without providing a prompt\n",
      "        >>> outputs = model.generate(max_length=40)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
      "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
      "        >>> document = (\n",
      "        ... \"at least two people were killed in a suspected bomb attack on a passenger bus \"\n",
      "        ... \"in the strife-torn southern philippines on monday , the military said.\"\n",
      "        ... )\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(document, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 independent sequences using beam search decoding (5 beams)\n",
      "        >>> # with T5 encoder-decoder model conditioned on short news article.\n",
      "        >>> outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
      "        >>> input_context = \"The dog\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate 3 candidates using sampling\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, num_return_sequences=3, do_sample=True)\n",
      "        >>> print(\"Generated:\", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"ctrl\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"ctrl\")\n",
      "        >>> # \"Legal\" is one of the control codes for ctrl\n",
      "        >>> input_context = \"Legal My neighbor is\"\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, repetition_penalty=1.2)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "    \n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
      "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
      "        >>> input_context = \"My cute dog\"\n",
      "        >>> # get tokens of words that should not be generated\n",
      "        >>> bad_words_ids = [tokenizer(bad_word, add_prefix_space=True).input_ids for bad_word in [\"idiot\", \"stupid\", \"shut up\"]]\n",
      "        >>> # encode input context\n",
      "        >>> input_ids = tokenizer(input_context, return_tensors=\"pt\").input_ids\n",
      "        >>> # generate sequences without allowing bad_words to be generated\n",
      "        >>> outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids)\n",
      "        >>> print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = tokenizer.encode(\"<pad>\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(input_ids = tokenized_text['input_ids'],\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=60,\n",
    "                                    max_length=150,\n",
    "                                    early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summarized text: \n",
      " i'm originally from Denver colorado but was raised right here in Knoxville. my elementary middle and high school are all within a 15 mile radius of the university of tennessee. I've been playing sports since I was six years old, which helped me become who I am today.\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm originally from Denver Colorado but was raised right here in Knoxville . My elementary middle and high school are all within a 15 mile radius of the University of Tennessee . I've been playing sports since I was six years old, which helped me become who I am today.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truecase.get_true_case(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
